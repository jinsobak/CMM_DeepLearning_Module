{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbtJOue2AhXdFcAjstBqvZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsobak/CMM_DeepLearning_Module/blob/SHY-code/autotuned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ucLb3a2EOU3h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import optuna\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "def prepare_data(csv_file):\n",
        "    # CSV 파일을 읽어들여 DataFrame으로 변환\n",
        "    all_data = pd.read_csv(csv_file, encoding='cp949')\n",
        "\n",
        "    # 특징 선택 (불필요한 열 제거 등)\n",
        "    selected_features = all_data.drop(columns=['품질상태'])  # 품질상태를 제외한 특징 선택\n",
        "\n",
        "    # 숫자 데이터만 선택\n",
        "    numeric_features = selected_features.select_dtypes(include=[np.number])\n",
        "\n",
        "    # 딥러닝의 입력 데이터와 정답 데이터 생성\n",
        "    X = numeric_features.values  # 입력 데이터\n",
        "    y = all_data['품질상태'].values  # 출력 데이터\n",
        "\n",
        "    # 테스트 데이터와 트레이닝 데이터로 분할\n",
        "    X_train, X_test_full, Y_train, Y_test_full = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_test, X_val, Y_test, Y_val = train_test_split(X_test_full, Y_test_full, test_size=0.5, random_state=42)\n",
        "\n",
        "    # 데이터 스케일링\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, X_val_scaled, Y_train, Y_test, Y_val, scaler, selected_features.columns\n",
        "\n",
        "\n",
        "# Objective 함수: Optuna가 하이퍼파라미터 튜닝에 사용\n",
        "def objective(trial):\n",
        "    # CSV 파일 경로\n",
        "    csv_file = '/content/data_jd_hd_delete_material_no_NTC_pca_component_7.csv'\n",
        "\n",
        "    # 데이터 전처리\n",
        "    X_train, X_test, X_val, y_train, y_test, Y_val, scaler, feature_columns = prepare_data(csv_file)\n",
        "\n",
        "    # 하이퍼파라미터 범위 정의\n",
        "    num_layers = trial.suggest_int('num_layers', 2, 7)  # 은닉층의 개수를 튜닝\n",
        "    hidden_size = trial.suggest_int('hidden_size', 4, 256)  # 각 은닉층의 뉴런 수\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)  # 학습률\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])  # 배치 크기\n",
        "    activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'sigmoid'])  # 활성화 함수 선택\n",
        "\n",
        "    # 특징과 레이블을 TensorFlow Dataset으로 변환\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)).batch(batch_size)\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).batch(batch_size)\n",
        "\n",
        "    # 모델 구성\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(hidden_size, activation=activation, input_shape=(X_train.shape[1],)))\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        model.add(layers.Dense(hidden_size, activation=activation))\n",
        "\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))  # 이진 분류를 위한 출력층\n",
        "\n",
        "    # 모델 컴파일\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Early Stopping 및 Learning Rate Scheduler 설정\n",
        "    early_stopping = EarlyStopping(min_delta=0.001, patience=10, restore_best_weights=True)\n",
        "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "    # 모델 학습\n",
        "    history = model.fit(train_dataset,\n",
        "                        epochs=50,\n",
        "                        callbacks=[early_stopping, lr_scheduler],\n",
        "                        validation_data=val_dataset,\n",
        "                        verbose=0)\n",
        "\n",
        "    # 모델 평가\n",
        "    loss, accuracy = model.evaluate(test_dataset, verbose=0)\n",
        "\n",
        "    # 예측 및 성능 평가\n",
        "    y_pred_prob = model.predict(X_test)\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "    # F1 스코어를 최대화하는 방향으로 튜닝\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    return f1\n",
        "\n",
        "# Optuna 스터디 생성 및 최적화 수행\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction='maximize')  # F1 스코어를 최대화\n",
        "    study.optimize(objective, n_trials=50)\n",
        "\n",
        "    # 최적의 하이퍼파라미터 출력\n",
        "    print(\"Best hyperparameters: \", study.best_params)\n",
        "\n",
        "    # 최적의 하이퍼파라미터로 모델 학습 및 평가\n",
        "    best_params = study.best_params\n",
        "\n",
        "    # CSV 파일 경로\n",
        "    csv_file = '/content/data_jd_hd_delete_material_no_NTC_pca_component_7.csv'\n",
        "\n",
        "    # 데이터 전처리\n",
        "    X_train, X_test, X_val, y_train, y_test, Y_val, scaler, feature_columns = prepare_data(csv_file)\n",
        "\n",
        "    # 최적의 하이퍼파라미터로 모델 구성\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(best_params['hidden_size'], activation=best_params['activation'], input_shape=(X_train.shape[1],)))\n",
        "\n",
        "    for _ in range(best_params['num_layers']):\n",
        "        model.add(layers.Dense(best_params['hidden_size'], activation=best_params['activation']))\n",
        "\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))  # 이진 분류를 위한 출력층\n",
        "\n",
        "    # 최적의 학습률로 모델 컴파일\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # 특징과 레이블을 TensorFlow Dataset으로 변환\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)).batch(best_params['batch_size'])\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(best_params['batch_size'])\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).batch(best_params['batch_size'])\n",
        "\n",
        "    # Early Stopping 및 Learning Rate Scheduler 설정\n",
        "    early_stopping = EarlyStopping(min_delta=0.001, patience=10, restore_best_weights=True)\n",
        "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "    # 모델 학습\n",
        "    history = model.fit(train_dataset,\n",
        "                        epochs=50,\n",
        "                        callbacks=[early_stopping, lr_scheduler],\n",
        "                        validation_data=val_dataset)\n",
        "\n",
        "    # 모델 평가\n",
        "    loss, accuracy = model.evaluate(test_dataset)\n",
        "    print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
        "\n",
        "    # 예측 결과\n",
        "    y_pred_prob = model.predict(X_test)\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    # 성능 지표\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    print(f'Accuracy: {acc}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'F1 Score: {f1}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KBdZkl2bVgc7"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}